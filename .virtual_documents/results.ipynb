





import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
from tqdm import tqdm
import seaborn as sns





!wget http://www.cs.cmu.edu/~ark/personas/data/MovieSummaries.tar.gz


! mkdir -p ./data/MovieSummaries
! tar xvf ./MovieSummaries.tar.gz --strip-components=1 -C ./data/MovieSummaries





! python ./src/scripts/basic_process_cmu.py \
    --data_dir ./data/MovieSummaries \
    --output_dir ./data/MovieSummaries \
    --process_movies_helper ./src/scripts/scripts_json_helpers/process_movies_helper.json \
    --process_actors_helper ./src/scripts/scripts_json_helpers/process_actors_helper.json \
    --process_ethnicities_helper ./src/scripts/scripts_json_helpers/process_ethnicities_helper.json





char_processed = pd.read_csv(
    "./data/MovieSummaries/character_processed.csv",
    parse_dates=["actor_date_of_birth", "movie_release_date"],
)





char_processed.isna().sum(axis=0) / char_processed.shape[0]





char_processed.isna().sum(axis=1).value_counts()





movies_processed = pd.read_csv(
    "./data/MovieSummaries/movie_processed.csv",
    parse_dates=["movie_release_date"],
)


movies_processed.isna().sum(axis=0) / movies_processed.shape[0]





all_plots_wikiids = set()

with open("./data/MovieSummaries/plot_summaries.txt") as f:
    lines = f.read().splitlines()
    for line in lines:
        wikiid, plot_summ = line.split("\t")
        if len(plot_summ) > 0:
            all_plots_wikiids.add(int(wikiid))

len(set(movies_processed["Wikipedia movie ID"].values.tolist()) - all_plots_wikiids) / len(movies_processed)








char_processed = char_processed.rename(columns={
    'Wikipedia movie ID': "wikipedia_movie_id",
    'Freebase movie ID': "fb_movie_id",
    'Character name': "character_name",
    'Actor gender': "actor_gender",
    'Actor height (in meters)': "actor_height",
    'Actor ethnicity (Freebase ID)': "fb_actor_eth_id",
    'Actor name': "actor_name",
    'Freebase character/actor map ID': "fb_char_actor_map_id",
    'Freebase character ID': "fb_char_id",
    'Freebase actor ID': "fb_actor_id",
})

char_processed





char_indep = char_processed[["actor_gender", "actor_height", "actor_name", "actor_date_of_birth", "fb_actor_id", "ethn_name", "race"]].drop_duplicates(subset=["fb_actor_id"])


char_indep.shape


char_indep["actor_gender"].value_counts()





char_indep.loc[char_indep["actor_height"] > 3, "actor_height"] = char_indep.loc[char_indep["actor_height"] <= 3, "actor_height"].max()


char_indep[["actor_height", "actor_gender"]].groupby(by="actor_gender").describe()


char_indep[['actor_height', 'actor_name']].sort_values(by='actor_height', ascending=True).head(10)





sns.boxplot(data=char_indep, x="actor_gender", y="actor_height", showfliers=False)





char_indep["race_gender"] = char_indep["race"] + "_" + char_indep["actor_gender"]

with plt.style.context("seaborn-v0_8-whitegrid"):
    plt.figure(figsize=(20, 5))

    sns.boxplot(
        data=char_indep,
        x="race_gender",
        y="actor_height",
        showfliers=False,
        order=char_indep["race_gender"].dropna().sort_values().unique()
    )
    plt.xticks(rotation=70)





char_indep_gender_by_race = char_indep[["actor_gender", "race"]].groupby("race")["actor_gender"].value_counts().unstack(fill_value=0).reset_index()
char_indep_gender_by_race["total"] = char_indep_gender_by_race[["F", "M"]].sum(axis=1)

char_indep_gender_by_race["F"] /= char_indep_gender_by_race["total"]
char_indep_gender_by_race["M"] /= char_indep_gender_by_race["total"]


char_indep_gender_by_race[["race", "M", "F"]].sort_values(by="M", ascending=True).plot(
    x="race",
    kind="barh",
    stacked=True,
    title="Gender by race",
    mark_right=True,
)





# Taken from https://www.kaggle.com/code/werooring/nfl-big-data-bowl-basic-eda-for-beginner
def write_percent(ax, total_size):
    '''Traverse the figure object and display the ratio at the top of the bar graph.'''
    for patch in ax.patches:
        height = patch.get_height() # Figure height (number of data)
        width = patch.get_width() # Figure width
        left_coord = patch.get_x() # The x-axis position on the left edge of the figure
        percent = height/total_size*100 # percent

        # Type text in the (x, y) coordinates
        ax.text(x=left_coord + width/2.0, # x-axis position
                y=height + total_size*0.001, # y-axis position
                s=f'{percent:1.1f}%', # Text
                ha='center') # in the middle


with plt.style.context('seaborn-v0_8'):
    plt.title("Race by percentage")
    ax = sns.countplot(x=char_indep.race, order=char_indep.race.value_counts().sort_values(ascending=False).index)
    write_percent(ax, len(char_indep.race.dropna()))
    plt.xlabel("Race")
    plt.xticks(rotation=90)
    plt.ylabel("Percentage")





with plt.style.context('seaborn-v0_8'):
    plt.figure(figsize=(12, 8))
    plt.title("Distribution of actors' year of birth")
    sns.kdeplot(char_indep.actor_date_of_birth.dt.year, label="All")
    sns.kdeplot(char_indep.query("actor_gender=='M'").actor_date_of_birth.dt.year, label="M")
    sns.kdeplot(char_indep.query("actor_gender=='F'").actor_date_of_birth.dt.year, label="F")
    plt.xticks(ticks=[i for i in range(1780, 2021, 20)])
    plt.legend()





movies_processed.rename(columns={
    "Wikipedia movie ID": "wikipedia_movie_id",
    "Freebase movie ID": "fb_movie_id",
    "Movie name": "movie_name",
    "Movie box office revenue": "revenue",
    "Movie runtime": "movie_runtime",
}, inplace=True)


movies_processed





movies_processed.sort_values(by="revenue", ascending=False).head(10)[["movie_name", "revenue"]]


movies_processed.sort_values(by="movie_runtime", ascending=False).head(10)[["movie_name", "movie_runtime"]]


with plt.style.context("seaborn-v0_8"):
    plt.figure(figsize=(12, 6))
    plt.subplot(1, 2, 1)
    plt.title("Revenue distribution")
    sns.kdeplot(movies_processed["revenue"])
    plt.subplot(1, 2, 2)
    plt.title("Revenue distribution (90th percentile)")
    sns.kdeplot(movies_processed["revenue"][movies_processed["revenue"] < movies_processed["revenue"].quantile(0.9)])





def get_distribution_from_list(data, column_of_list):
    data = data.copy().dropna(subset=[column_of_list])
    list_to_values = {}
    for _, row in tqdm(data.iterrows(), total=data.shape[0]):
        for el in eval(row[column_of_list]):
            if not el in list_to_values:
                list_to_values[el] = 0
            list_to_values[el] += 1
    result = pd.DataFrame(pd.Series(list_to_values)).reset_index().rename(columns={0: "value", "index": "smth"})
    result.sort_values("value", ascending=False, inplace=True)
    result_cut = result.iloc[:50]
    plt.bar(result_cut["smth"], result_cut["value"])
    write_percent(plt.gca(), result["value"].sum())
    plt.xticks(rotation=90)

for column in ["genres", "languages", "countries"]:
    plt.figure(figsize=(20, 6))
    plt.title(f"Distribution of {column}")
    get_distribution_from_list(movies_processed, column)
    plt.show()


def list_to_values(data, column_of_list, column_value, use_sum):
    data = data.copy()
    list_to_values = {}
    for _, row in tqdm(data.iterrows(), total=data.shape[0]):
        value = row[column_value]
        if value is None:
            continue
        for el in eval(row[column_of_list]):
            if not el in list_to_values:
                list_to_values[el] = []
            list_to_values[el].append(value)

    for key in list_to_values:
        if use_sum:
            list_to_values[key] = np.sum(list_to_values[key])
        else:
            list_to_values[key] = np.median(list_to_values[key])
    return pd.DataFrame(pd.Series(list_to_values)).reset_index().rename(columns={0: "value", "index": "smth"})

def get_distribution_by(data, column=None, func_to_apply=None, column_to_apply=None, title="", use_sum=False):
    '''
    This function helps to get the distribution of the data by:
    1. Year
    2. County
    3. Genre
    4. Language

    It also plots the distribution of the data by the above-mentioned categories.
    '''
    data = data.copy().dropna(subset=[column])
    if not func_to_apply is None:
        data[column] = data[column_to_apply].apply(func_to_apply)

    # by year
    data["year"] = data["movie_release_date"].dt.year
    if use_sum:
        by_year = pd.DataFrame(data.groupby("year")[column].sum()).reset_index()
    else:
        by_year = pd.DataFrame(data.groupby("year")[column].mean()).reset_index()

    # by country
    by_country = list_to_values(data, "countries", column, use_sum=use_sum)

    # by genre
    by_genre = list_to_values(data, "genres", column, use_sum=use_sum)

    # by language
    by_language = list_to_values(data, "languages", column, use_sum=use_sum)

    with plt.style.context("seaborn-v0_8"):
        plt.figure(figsize=(15, 30))
        plt.suptitle(title)
        for i, by_smth, name in zip(
                [0, 1, 2, 3],
                [by_year, by_country, by_genre, by_language], ["year", "country", "genre", "language"]
            ):
            plt.subplot(4, 1, i + 1)
            plt.title(name)
            if i == 0:
                by_smth = by_smth.sort_values("year")
                xname = "year"
                yname = column
            else:
                by_smth = by_smth.sort_values("value", ascending=False)
                xname = "smth"
                yname = "value"
                by_smth = by_smth.iloc[:50]
            plt.plot(by_smth[xname], by_smth[yname])
            plt.xticks(rotation=90, ha='right')
        plt.tight_layout(rect=[0, 0.01, 1, 0.95])
    return by_year, by_country, by_genre, by_language





get_distribution_by(movies_processed, "revenue", title="Revenue by:", use_sum=True);








get_distribution_by(movies_processed, "revenue", title="Revenue by:", use_sum=False);





get_distribution_by(movies_processed, "movie_runtime", title="Runtime by:");





with plt.style.context("seaborn-v0_8"):
    movies_processed["year"] = movies_processed["movie_release_date"].dt.year
    movies_processed["language_num"] = movies_processed["languages"].apply(lambda x: len(eval(x)))
    movies_processed[["year", "language_num"]].groupby("year")["language_num"].mean().plot(x="year", y="language_num", title="Number of languages by year")
    plt.xticks(ticks=[i for i in range(1890, 2021, 20)]);





with plt.style.context("seaborn-v0_8"):
    movies_processed["year"] = movies_processed["movie_release_date"].dt.year
    movies_processed["genre_num"] = movies_processed["genres"].apply(lambda x: len(eval(x)))
    movies_processed[["year", "genre_num"]].groupby("year")["genre_num"].mean().plot(x="year", y="genre_num", title="Number of genres by year")
    plt.xticks(ticks=[i for i in range(1890, 2021, 20)]);





with plt.style.context("seaborn-v0_8"):
    movies_processed["year"] = movies_processed["movie_release_date"].dt.year
    movies_processed["movie_name_len"] = movies_processed["movie_name"].apply(lambda x: len(x.split(" ")))
    movies_processed[["year", "movie_name_len"]].groupby("year")["movie_name_len"].mean().plot(x="year", y="movie_name_len", title="Movie name length by year")
    plt.xticks(ticks=[i for i in range(1890, 2021, 20)]);





with plt.style.context("seaborn-v0_8"):
    movies_processed["year"] = movies_processed["movie_release_date"].dt.year
    movies_processed[["year", "movie_name"]].groupby("year")["movie_name"].count().plot(x="year", y="movine_name", title="Number of films by year")
    plt.xticks(ticks=[i for i in range(1890, 2021, 20)]);





with plt.style.context("seaborn-v0_8"):
    plt.title("Revenue by number of languages")
    plt.scatter(movies_processed["language_num"], movies_processed["revenue"])
    plt.xlabel("Number of languages")
    plt.ylabel("Revenue");





correspondeces_genre_country = {}

all_genres = set([i for j in movies_processed["genres"].values for i in eval(j)])
all_countries = set([i for j in movies_processed["countries"].values for i in eval(j)])

for g in all_genres:
    for c in all_countries:
        if not g in correspondeces_genre_country:
            correspondeces_genre_country[g] = {}
        correspondeces_genre_country[g][c] = 0

for _, row in tqdm(movies_processed[["genres", "countries"]].dropna().iterrows(), total=len(movies_processed)):
    genres_curr = eval(row['genres'])
    countries_curr = eval(row['countries'])
    for g in genres_curr:
        for c in countries_curr:
            correspondeces_genre_country[g][c] += 1





correspondeces_genre_country_pd = pd.DataFrame(correspondeces_genre_country)
normalized_corres_gc = correspondeces_genre_country_pd.values / correspondeces_genre_country_pd.values.sum(axis=1)[:, None]
correspondeces_genre_country_pd["most_popular"] = correspondeces_genre_country_pd.columns[normalized_corres_gc.argmax(axis=1)]
print(correspondeces_genre_country_pd.sort_values("most_popular")["most_popular"].to_string())





data_merged_raw = char_processed.merge(movies_processed.drop(columns=["fb_movie_id", "movie_release_date"]), on="wikipedia_movie_id")


# we do not need every column in our current analysis
data_merged = data_merged_raw[[
    'wikipedia_movie_id', 'character_name', 'actor_gender',
    'actor_height', 'fb_actor_eth_id', 'actor_name', 'fb_char_actor_map_id',
    'fb_char_id', 'fb_actor_id', 'actor_date_of_birth',
    'movie_release_date', 'ethn_name', 'race', 'movie_name', 'revenue',
    'movie_runtime', 'languages', 'countries_old', 'countries', 'genres',
    'year', 'language_num', 'genre_num', 'movie_name_len'
]]





from scipy.stats import spearmanr

revenue_by_actors_count = data_merged[["movie_name", "character_name"]]\
    .groupby(["movie_name"])\
    .count().reset_index().rename(columns={"character_name": "actors_count"})\
    .merge(movies_processed[["movie_name", "revenue"]], on="movie_name")[["actors_count", "revenue"]].dropna()

print(spearmanr(revenue_by_actors_count["actors_count"], revenue_by_actors_count["revenue"]))
revenue_by_actors_count.plot(x="actors_count", y="revenue", kind="scatter", alpha=0.1)
plt.xlim(0, 50)
plt.ylim(0, 1e9)
plt.title("Revenue by number of actors")
plt.xlabel("Number of actors")
plt.ylabel("Revenue");





correspondeces_race_country = {}

all_races = data_merged["race"].dropna().unique()
all_countries = set([i for j in data_merged["countries"].values for i in eval(j)])

for r in all_races:
    for c in all_countries:
        if not r in correspondeces_race_country:
            correspondeces_race_country[r] = {}
        correspondeces_race_country[r][c] = 0

for _, row in tqdm(data_merged[["race", "countries"]].dropna().iterrows(), total=len(data_merged)):
    race = row['race']
    for c in eval(row['countries']):
        correspondeces_race_country[race][c] += 1


correspondeces_race_country_pd = pd.DataFrame(correspondeces_race_country)
# normalize by sum of other races
normalized_corres = correspondeces_race_country_pd.values / (1e-6 + correspondeces_race_country_pd.values.sum(axis=1)[:, None])
sns.heatmap(normalized_corres)
plt.xticks(ticks=np.arange(0, 12) + 0.5, labels=correspondeces_race_country_pd.columns, rotation=90);





correspondeces_race_country_pd["most_popular"] = correspondeces_race_country_pd.columns[normalized_corres.argmax(axis=1)]
print(correspondeces_race_country_pd.sort_values("most_popular")["most_popular"].to_string())





import pandas as pd

df = pd.read_csv('./data/MovieSummaries/character_processed_enriched.csv')
df = df[['character_name', 'actor_name', 'character_description', 'actor_description']][1:]
df





df[~df.actor_description.isna()]





df[~df.character_description.isna()]











! python ./src/scripts/movie_metadata_selection.py


df = pd.read_csv('./data/MovieSummaries/movie_processed.csv')
df.columns.values[0] = 'wiki_movie_id'
df.columns.values[1] = 'freebase_movie_id'
df.columns.values[2] = 'movie_name'
df.columns.values[3] = 'revenue'
df.columns.values[4] = 'movie_runtime'
#df = pd.read_csv('./data/MovieSummaries/movie_processed_enriched.csv')
#df = df[['movie_name', 'release_date', 'plot_summary', 'genres', 'cast']][1:]
df.head(3)


df_fb = pd.read_json('./data/MovieSummaries/movies_freebase.json')
df_fb.rename(columns={'discription': 'freebase_plot_summary', 'fb_movie_id': 'freebase_movie_id'}, inplace=True)
df_fb.head(3)


movies_merged_df = pd.merge(df, df_fb, on='freebase_movie_id', how='inner')
movies_merged_df.head(3)


columns_to_drop = ['wiki_en_path', 'wiki_en_id', 'wiki_en_title', 'wiki_en_page']
v2_movie_processed = movies_merged_df.drop(columns=columns_to_drop)
v2_movie_processed.reset_index(drop=True, inplace=True)


v2_movie_processed.to_csv("./data/MovieSummaries/v2_movie_processed.csv", index = False)
v2_movie_processed.head(3)


! python ./src/scripts/movie_metadata_selection.py --input_file="v2_movie_processed.csv" --output_file_name="v2_movie_processed_enriched.csv"


df['plot_summary'].count()











import os
import sys

GLOUD_CREDENTIALS = "<path to credentials.json>"
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = GLOUD_CREDENTIALS





import pandas as pd

test_df = pd.read_csv("./data/persona_identification/persona_identification_testset.csv", index_col="index")
test_df





from langchain_core.rate_limiters import InMemoryRateLimiter
from src.models.persona_identification import PersonaIdentification
from langchain.cache import InMemoryCache
from langchain.globals import set_llm_cache
from langchain_redis import RedisCache

# to cache requests to LM, consider using remote redis server
cache = InMemoryCache()
# cache = RedisCache(redis_url="redis://localhost:6379")
set_llm_cache(cache)

rate_limiter = InMemoryRateLimiter(
    requests_per_second=4.99 / 60,  # free google cloud account allows 5 requests per minute
    check_every_n_seconds=0.1,
    max_bucket_size=5,
)

pi = PersonaIdentification(
    config_path="./src/models/persona_identification_config.yaml",
    cache=cache,
    rate_limiter=rate_limiter,
)

test_df_results = pi.batch(test_df.to_dict(orient="records"))


test_df_results[0]


from src.models.persona_identification import persona_lowercase
from sklearn.metrics import precision_score, recall_score, f1_score

test_df["model_predictions"] = [persona_lowercase(res.content) for res in test_df_results]
test_df["is_prediction_valid"] = [1 if res.response_metadata["parsing_success"] else 0 for res in test_df_results]

print("Not valid predictions:")
display(test_df[test_df.is_prediction_valid == 0])
print(f"Prediction accuracy: {(test_df.model_predictions == test_df.persona).mean(): .3f}")

micro_f1 = f1_score(test_df['persona'],test_df['model_predictions'], average='micro')
macro_f1 = f1_score(test_df['persona'],test_df['model_predictions'], average='macro')
print(f"Micro f1: {micro_f1:.2f}")
print(f"Macro f1: {macro_f1:.2f}")

test_df.to_excel("model_predictions_analysis.xlsx");






